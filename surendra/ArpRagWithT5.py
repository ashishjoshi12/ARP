# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gV-kH4aWwqjWmeOyIrVypwX5DBFhiYB_
"""

# Install necessary libraries
# !pip install pdfplumber sentence-transformers faiss-cpu faiss-cpu transformers

import pdfplumber
import faiss
import numpy as np
import re
from sentence_transformers import SentenceTransformer

# Step 1: Extract Text and Tables from PDF


def extract_text_tables(pdf_path):
    """
    Extract text and tables from a PDF file.
    Returns the full text and tables as lists.
    """
    with pdfplumber.open(pdf_path) as pdf:
        all_text = []
        all_tables = []
        for page in pdf.pages:
            all_text.append(page.extract_text())
            tables = page.extract_tables()
            all_tables.extend(tables)
        return "\n".join(all_text), all_tables

# Define the file path to the PDF (provide the path here)
pdf_path = "D:/surendra/annualReview/gxocompany.pdf" 

# Extract text and tables
text_data, table_data = extract_text_tables(pdf_path)

# Display extracted text
print("Extracted Text (First 500 characters):\n", text_data[:500])

# Display the first table if available
if table_data:
    print("\nExtracted Table (First Table):\n", table_data[0])

def clean_text(text):
       """
       Cleans the input text by removing unwanted characters and patterns.
       """
       # Remove special characters and punctuation (except for periods and commas)
       text = re.sub(r"[^a-zA-Z0-9.,\s]", "", text)
       # Remove extra whitespace
       text = re.sub(r"\s+", " ", text)
       # Convert to lowercase
       text = text.lower()
       return text

# Step 2: Generate Embeddings using SentenceTransformers

# Load SentenceTransformer model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Clean the extracted text
#cleaned_text = clean_text(text_data)

# Split text into smaller chunks (e.g., paragraphs or lines)
text_chunks = text_data.split('\n')

# Generate embeddings for the text chunks
embeddings = embedding_model.encode(text_chunks, convert_to_numpy=True)

# Step 3: Setup FAISS Index for Retrieval

# Create FAISS index
dimension = embeddings.shape[1]
faiss_index = faiss.IndexFlatL2(dimension)  # L2 distance
faiss_index.add(embeddings)  # Add embeddings to the index

# Function to retrieve relevant chunks
def retrieve(query, k=5):
    """
    Retrieve the top-k relevant text chunks for a given query.
    """
    query_embedding = embedding_model.encode([query], convert_to_numpy=True)
    distances, indices = faiss_index.search(query_embedding, k)
    return [text_chunks[i] for i in indices[0]]

# Step 4: Generate Commentary using T5
from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load T5 model and tokenizer
t5_model = T5ForConditionalGeneration.from_pretrained('T5-large')
t5_tokenizer = T5Tokenizer.from_pretrained('T5-large')

# Test retrieval
query = "how is the balance sheet"
retrieved_chunks = retrieve(query)
print("\nRetrieved Chunks:\n", retrieved_chunks)

def generate_commentary(context, query):
    """
    Generate a response based on the retrieved context and user query.
    """
    input_text = f"question: {query} context: {context}"
    input_ids = t5_tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
    outputs = t5_model.generate(input_ids, max_length=200, num_beams=4, early_stopping=True)
    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)

# Generate commentary based on retrieved chunks
context = " ".join(retrieved_chunks)
response = generate_commentary(context, query)
print("\nGenerated Commentary:\n", response)

# Step 5: Query Multiple Times
while True:
    user_query = input("\nEnter your query (type 'exit' to stop): ")
    if user_query.lower() == 'exit':
        break
    retrieved_chunks = retrieve(user_query)
    context = " ".join(retrieved_chunks)
    response = generate_commentary(context, user_query)
    print("\nGenerated Commentary:\n", response)